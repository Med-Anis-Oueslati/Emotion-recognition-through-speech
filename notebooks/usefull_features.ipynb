{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Conv1D,\n",
    "    Activation,\n",
    "    Dropout,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Input,\n",
    ")\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/processed/audio_files_tess.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# Function to recursively get all audio files from a directory\n",
    "def get_audio_files(base_path):\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "\n",
    "# Base path for TESS dataset\n",
    "base_path_TESS = \"../data/raw/TESS_Toronto_emotional_speech_set_data\"\n",
    "\n",
    "# Get all audio files from TESS datasets\n",
    "audio_files_tess = get_audio_files(base_path_TESS)\n",
    "# Save audio files paths\n",
    "if not os.path.exists(\"../data/processed/audio_files_tess.txt\"):\n",
    "    with open(\"../data/processed/audio_files_tess.txt\", \"w\") as f:\n",
    "        for item in audio_files_tess:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    print(\"../data/processed/audio_files_tess.txt already exists is created\")\n",
    "else:\n",
    "    print(\"../data/processed/audio_files_tess.txt already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "No data left in file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 122\u001b[0m\n\u001b[0;32m    120\u001b[0m     np\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/processed/features_labels_datasets.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, datasets)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 122\u001b[0m     datasets \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/processed/features_labels_datasets.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets loaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Med Anis Oueslati\\Desktop\\internship\\EmotionRecognition\\.venv\\Lib\\site-packages\\numpy\\lib\\npyio.py:436\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    434\u001b[0m magic \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mread(N)\n\u001b[0;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[1;32m--> 436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data left in file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    437\u001b[0m \u001b[38;5;66;03m# If the file size is less than N, we need to make sure not\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# to seek past the beginning of the file\u001b[39;00m\n\u001b[0;32m    439\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(N, \u001b[38;5;28mlen\u001b[39m(magic)), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# back-up\u001b[39;00m\n",
      "\u001b[1;31mEOFError\u001b[0m: No data left in file"
     ]
    }
   ],
   "source": [
    "# Function to extract mfcc, chroma, mel, and contrast features from audio files\n",
    "def extract_features(\n",
    "    file_path,\n",
    "    include_mfcc=True,\n",
    "    include_chroma=True,\n",
    "    include_mel=True,\n",
    "    include_contrast=True,\n",
    "    sample_rate=22050,\n",
    "):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=sample_rate)\n",
    "        features = []\n",
    "\n",
    "        if include_mfcc:\n",
    "            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "            features.append(np.mean(mfccs, axis=1))\n",
    "\n",
    "        if include_chroma:\n",
    "            chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "            features.append(np.mean(chroma, axis=1))\n",
    "\n",
    "        if include_mel:\n",
    "            mel = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "            features.append(np.mean(mel, axis=1))\n",
    "\n",
    "        if include_contrast:\n",
    "            contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
    "            features.append(np.mean(contrast, axis=1))\n",
    "\n",
    "        return np.hstack(features)\n",
    "    except Exception:\n",
    "        print(f\"Error encountered while parsing file: {file_path}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load audio files from the text file\n",
    "audio_files_tess = []\n",
    "with open(\"../data/processed/audio_files_tess.txt\", \"r\") as file:\n",
    "    audio_files_tess = [line.strip() for line in file.readlines()]\n",
    "\n",
    "\n",
    "# Function to create datasets excluding one feature each time\n",
    "def create_datasets():\n",
    "    features_labels = []\n",
    "\n",
    "    feature_sets = {\n",
    "        \"all_features\": (True, True, True, True),\n",
    "        \"no_mfcc\": (False, True, True, True),\n",
    "        \"no_chroma\": (True, False, True, True),\n",
    "        \"no_mel\": (True, True, False, True),\n",
    "        \"no_contrast\": (True, True, True, False),\n",
    "    }\n",
    "    \n",
    "\n",
    "    for set_name, (mfcc, chroma, mel, contrast) in feature_sets.items():\n",
    "        features = []\n",
    "        labels = []\n",
    "        label_map_tess = {\n",
    "            \"OAF_angry\": 4,\n",
    "            \"OAF_disgust\": 6,\n",
    "            \"OAF_Fear\": 5,\n",
    "            \"OAF_happy\": 2,\n",
    "            \"OAF_Pleasant_surprise\": 7,\n",
    "            \"OAF_Sad\": 3,\n",
    "            \"OAF_neutral\": 0,\n",
    "            \"YAF_angry\": 4,\n",
    "            \"YAF_disgust\": 6,\n",
    "            \"YAF_fear\": 5,\n",
    "            \"YAF_happy\": 2,\n",
    "            \"YAF_pleasant_surprised\": 7,\n",
    "            \"YAF_sad\": 3,\n",
    "            \"YAF_neutral\": 0,\n",
    "        }\n",
    "        label_map_ravdess = {\n",
    "            \"01\": 0,\n",
    "            \"02\": 1,\n",
    "            \"03\": 2,\n",
    "            \"04\": 3,\n",
    "            \"05\": 4,\n",
    "            \"06\": 5,\n",
    "            \"07\": 6,\n",
    "            \"08\": 7,\n",
    "        }\n",
    "        for idx, file in enumerate(audio_files_tess, start=1):\n",
    "            feature = extract_features(\n",
    "                file,\n",
    "                include_mfcc=mfcc,\n",
    "                include_chroma=chroma,\n",
    "                include_mel=mel,\n",
    "                include_contrast=contrast,\n",
    "            )\n",
    "            if feature is not None:\n",
    "                features.append(feature)\n",
    "                if \"audio_speech_actors_01-24\" in file:\n",
    "                    # Extract label from RAVDESS file name\n",
    "                    label = file.split(os.sep)[-1].split(\"-\")[2]\n",
    "                    labels.append(label_map_ravdess[label])\n",
    "                else:\n",
    "                    # Extract label from TESS file path\n",
    "                    emotion = file.split(os.sep)[-2]\n",
    "                    if emotion in label_map_tess:\n",
    "                        labels.append(label_map_tess[emotion])\n",
    "                    else:\n",
    "                        print(f\"Skipping {file} with unrecognized emotion: {emotion}\")\n",
    "                        features.pop()  # Remove the feature if label is not recognized\n",
    "            print(\n",
    "                f\"Processing file {idx} of {len(audio_files_tess)} for set {set_name}\"\n",
    "            )\n",
    "\n",
    "        features = np.array(features)\n",
    "        labels = np.array(labels)\n",
    "        features_labels.append((features, labels, set_name))\n",
    "\n",
    "    return features_labels\n",
    "\n",
    "\n",
    "# Create and save datasets\n",
    "if not os.path.exists(\"../data/processed/features_labels_datasets.npy\"):\n",
    "    datasets = create_datasets()\n",
    "    np.save(\"../data/processed/features_labels_datasets.npy\", datasets)\n",
    "else:\n",
    "    datasets = np.load(\n",
    "        \"../data/processed/features_labels_datasets.npy\", allow_pickle=True\n",
    "    )\n",
    "    print(\"Datasets loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 81\u001b[0m\n\u001b[0;32m     77\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model for each dataset\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m features, labels, set_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdatasets\u001b[49m:\n\u001b[0;32m     82\u001b[0m     train_evaluate_model(features, labels, set_name)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to train and evaluate the model for each dataset\n",
    "def train_evaluate_model(features, labels, set_name):\n",
    "    print(f\"Training and evaluating for feature set: {set_name}\")\n",
    "    # Split the dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Reshape data to fit the model: (num_samples, timesteps, num_features)\n",
    "    X_train = np.expand_dims(X_train, axis=-1)\n",
    "    X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # First convolutional layer with 64 filters, kernel size 5, 'same' padding, and ReLU activation\n",
    "    model.add(\n",
    "        Conv1D(64, 5, padding=\"same\", input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    )\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "\n",
    "    # Second convolutional layer with 128 filters, kernel size 5, 'same' padding, and ReLU activation\n",
    "    model.add(Conv1D(128, 5, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(MaxPooling1D(pool_size=4))\n",
    "\n",
    "    # Third convolutional layer with 256 filters, kernel size 5, 'same' padding, and ReLU activation\n",
    "    model.add(Conv1D(256, 5, padding=\"same\"))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    # Flatten the output for the dense layer\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Dense layer with 8 units and softmax activation for multi-class classification\n",
    "    model.add(Dense(8))\n",
    "    model.add(Activation(\"softmax\"))\n",
    "\n",
    "    # Define the RMSprop optimizer with a lower learning rate\n",
    "    opt = RMSprop(learning_rate=0.001)\n",
    "\n",
    "    # Compile the model with categorical crossentropy loss and the defined optimizer\n",
    "    model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test accuracy for {set_name}: {score[1]}\")\n",
    "\n",
    "    # Generate classification report and confusion matrix\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    print(classification_report(y_test, y_pred_classes))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    confusion_mtx = confusion_matrix(y_test, y_pred_classes)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sn.heatmap(confusion_mtx, annot=True, fmt=\"d\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(f\"Confusion Matrix for {set_name}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Train and evaluate the model for each dataset\n",
    "for features, labels, set_name in datasets:\n",
    "    train_evaluate_model(features, labels, set_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
