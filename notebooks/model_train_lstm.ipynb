{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras.layers import (\n",
    "    Conv1D,\n",
    "    Activation,\n",
    "    Dropout,\n",
    "    MaxPooling1D,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Input,\n",
    "    LSTM,\n",
    "    GRU,\n",
    ")\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from IPython.display import clear_output\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/processed/audio_files.txt already exists\n"
     ]
    }
   ],
   "source": [
    "# Function to recursively get all audio files from a directory\n",
    "def get_audio_files(base_path):\n",
    "    audio_files = []\n",
    "    for root, _, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".wav\"):\n",
    "                audio_files.append(os.path.join(root, file))\n",
    "    return audio_files\n",
    "\n",
    "\n",
    "# Base path for RAVDESS dataset\n",
    "base_path_RAVDESS = \"../data/raw/audio_speech_actors_01-24\"\n",
    "# Base path for TESS dataset\n",
    "base_path_TESS = \"../data/raw/TESS_Toronto_emotional_speech_set_data\"\n",
    "\n",
    "# Get all audio files from RAVDESS and TESS datasets\n",
    "audio_files_ravdess = get_audio_files(base_path_RAVDESS)\n",
    "audio_files_tess = get_audio_files(base_path_TESS)\n",
    "audio_files = audio_files_ravdess + audio_files_tess\n",
    "# Save audio files paths\n",
    "if not os.path.exists(\"../data/processed/audio_files.txt\"):\n",
    "    with open(\"../data/processed/audio_files.txt\", \"w\") as f:\n",
    "        for item in audio_files:\n",
    "            f.write(\"%s\\n\" % item)\n",
    "    print(\"../data/processed/audio_files.txt already exists is created\")\n",
    "else:\n",
    "    print(\"../data/processed/audio_files.txt already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract mfcc, chroma, mel, and contrast features from audio files\n",
    "def extract_features(file_path, sample_rate=22050):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=sample_rate)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "        mel = librosa.feature.melspectrogram(y=audio, sr=sr)\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
    "        features = np.hstack(\n",
    "            (\n",
    "                np.mean(mfccs, axis=1),\n",
    "                np.mean(chroma, axis=1),\n",
    "                np.mean(mel, axis=1),\n",
    "                np.mean(contrast, axis=1),\n",
    "            )\n",
    "        )\n",
    "        return features\n",
    "    except Exception:\n",
    "        print(f\"Error encountered while parsing file: {file_path}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load audio files from the text file\n",
    "audio_files = []\n",
    "with open(\"../data/processed/audio_files.txt\", \"r\") as file:\n",
    "    audio_files = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Extract features from all audio files\n",
    "features = []\n",
    "labels = []\n",
    "label_map_tess = {\n",
    "    \"OAF_angry\": 4,\n",
    "    \"OAF_disgust\": 6,\n",
    "    \"OAF_Fear\": 5,\n",
    "    \"OAF_happy\": 2,\n",
    "    \"OAF_Pleasant_surprise\": 7,\n",
    "    \"OAF_Sad\": 3,\n",
    "    \"OAF_neutral\": 0,\n",
    "    \"YAF_angry\": 4,\n",
    "    \"YAF_disgust\": 6,\n",
    "    \"YAF_fear\": 5,\n",
    "    \"YAF_happy\": 2,\n",
    "    \"YAF_pleasant_surprised\": 7,\n",
    "    \"YAF_sad\": 3,\n",
    "    \"YAF_neutral\": 0,\n",
    "}\n",
    "label_map_ravdess = {\n",
    "    \"01\": 0,\n",
    "    \"02\": 1,\n",
    "    \"03\": 2,\n",
    "    \"04\": 3,\n",
    "    \"05\": 4,\n",
    "    \"06\": 5,\n",
    "    \"07\": 6,\n",
    "    \"08\": 7,\n",
    "}\n",
    "if not os.path.exists(\"../data/processed/features.npy\"):\n",
    "    for idx, file in enumerate(audio_files, start=1):\n",
    "        feature = extract_features(file)\n",
    "        if feature is not None:\n",
    "            features.append(feature)\n",
    "            if \"audio_speech_actors_01-24\" in file:\n",
    "                # Extract label from RAVDESS file name\n",
    "                label = file.split(os.sep)[-1].split(\"-\")[2]\n",
    "                labels.append(label_map_ravdess[label])\n",
    "            else:\n",
    "                # Extract label from TESS file path\n",
    "                emotion = file.split(os.sep)[-2]\n",
    "                if emotion in label_map_tess:\n",
    "                    labels.append(label_map_tess[emotion])\n",
    "                else:\n",
    "                    print(f\"Skipping {file} with unrecognized emotion: {emotion}\")\n",
    "                    features.pop()  # Remove the feature if label is not recognized\n",
    "            print(f\"Processing file {idx} of {len(audio_files)}\")\n",
    "\n",
    "    print(\"Feature extraction complete.\")\n",
    "    features = np.array(features)\n",
    "    np.save(\"../data/processed/features.npy\", features)\n",
    "    labels = np.array(labels)\n",
    "    np.save(\"../data/processed/labels.npy\", labels)\n",
    "else:\n",
    "    # Load features and labels\n",
    "    features = np.load(\"../data/processed/features.npy\")\n",
    "    labels = np.load(\"../data/processed/labels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the features and labels into numpy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Reshape data to fit the model: (num_samples, timesteps, num_features)\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Med Anis Oueslati\\Desktop\\internship\\EmotionRecognition\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initialize a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM layer with 64 units\n",
    "# Input shape is (num_features, 1)\n",
    "model.add(\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2]))\n",
    ")\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Second LSTM layer with 128 units\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Third LSTM layer with 256 units\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Dense layer with 8 units and softmax activation for multi-class classification\n",
    "model.add(Dense(8))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# Define the RMSprop optimizer with a lower learning rate\n",
    "opt = RMSprop(learning_rate=0.001)\n",
    "\n",
    "# Compile the model with sparse categorical crossentropy loss and the defined optimizer\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 222ms/step - accuracy: 0.1486 - loss: 2.0454 - val_accuracy: 0.2429 - val_loss: 1.8699\n",
      "Epoch 2/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 209ms/step - accuracy: 0.2444 - loss: 1.8886 - val_accuracy: 0.2512 - val_loss: 1.8930\n",
      "Epoch 3/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 210ms/step - accuracy: 0.2902 - loss: 1.7194 - val_accuracy: 0.3467 - val_loss: 1.5845\n",
      "Epoch 4/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 214ms/step - accuracy: 0.3578 - loss: 1.5655 - val_accuracy: 0.3644 - val_loss: 1.6034\n",
      "Epoch 5/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 243ms/step - accuracy: 0.3582 - loss: 1.5370 - val_accuracy: 0.3785 - val_loss: 1.4637\n",
      "Epoch 6/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 227ms/step - accuracy: 0.4042 - loss: 1.4777 - val_accuracy: 0.4080 - val_loss: 1.4322\n",
      "Epoch 7/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 225ms/step - accuracy: 0.4338 - loss: 1.3955 - val_accuracy: 0.3986 - val_loss: 1.4864\n",
      "Epoch 8/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 278ms/step - accuracy: 0.4693 - loss: 1.3394 - val_accuracy: 0.5318 - val_loss: 1.1990\n",
      "Epoch 9/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 282ms/step - accuracy: 0.4976 - loss: 1.3025 - val_accuracy: 0.4894 - val_loss: 1.2319\n",
      "Epoch 10/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 273ms/step - accuracy: 0.4978 - loss: 1.2441 - val_accuracy: 0.5767 - val_loss: 1.0820\n",
      "Epoch 11/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 226ms/step - accuracy: 0.5295 - loss: 1.1941 - val_accuracy: 0.5189 - val_loss: 1.2107\n",
      "Epoch 12/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 238ms/step - accuracy: 0.5431 - loss: 1.1891 - val_accuracy: 0.5613 - val_loss: 1.1290\n",
      "Epoch 13/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 239ms/step - accuracy: 0.5668 - loss: 1.1258 - val_accuracy: 0.5637 - val_loss: 1.0884\n",
      "Epoch 14/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 302ms/step - accuracy: 0.5770 - loss: 1.0955 - val_accuracy: 0.3007 - val_loss: 2.0955\n",
      "Epoch 15/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 303ms/step - accuracy: 0.5141 - loss: 1.2924 - val_accuracy: 0.5861 - val_loss: 1.0830\n",
      "Epoch 16/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 264ms/step - accuracy: 0.5981 - loss: 1.0464 - val_accuracy: 0.5943 - val_loss: 1.0777\n",
      "Epoch 17/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 231ms/step - accuracy: 0.5783 - loss: 1.0775 - val_accuracy: 0.6250 - val_loss: 0.9689\n",
      "Epoch 18/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 321ms/step - accuracy: 0.6107 - loss: 1.0024 - val_accuracy: 0.6604 - val_loss: 0.9002\n",
      "Epoch 19/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 388ms/step - accuracy: 0.6234 - loss: 0.9787 - val_accuracy: 0.6792 - val_loss: 0.8758\n",
      "Epoch 20/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 380ms/step - accuracy: 0.6389 - loss: 0.9123 - val_accuracy: 0.6179 - val_loss: 0.9803\n",
      "Epoch 21/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 296ms/step - accuracy: 0.6493 - loss: 0.9092 - val_accuracy: 0.6851 - val_loss: 0.8450\n",
      "Epoch 22/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 335ms/step - accuracy: 0.6444 - loss: 0.9060 - val_accuracy: 0.6804 - val_loss: 0.8400\n",
      "Epoch 23/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 362ms/step - accuracy: 0.6817 - loss: 0.8268 - val_accuracy: 0.6828 - val_loss: 0.8254\n",
      "Epoch 24/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 288ms/step - accuracy: 0.6645 - loss: 0.8667 - val_accuracy: 0.6922 - val_loss: 0.8111\n",
      "Epoch 25/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 316ms/step - accuracy: 0.7110 - loss: 0.7886 - val_accuracy: 0.6981 - val_loss: 0.7959\n",
      "Epoch 26/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 291ms/step - accuracy: 0.6969 - loss: 0.7891 - val_accuracy: 0.6946 - val_loss: 0.7925\n",
      "Epoch 27/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 287ms/step - accuracy: 0.6902 - loss: 0.8237 - val_accuracy: 0.6922 - val_loss: 0.8113\n",
      "Epoch 28/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 289ms/step - accuracy: 0.7104 - loss: 0.7733 - val_accuracy: 0.6733 - val_loss: 0.8820\n",
      "Epoch 29/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 290ms/step - accuracy: 0.7141 - loss: 0.7530 - val_accuracy: 0.7028 - val_loss: 0.7589\n",
      "Epoch 30/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 287ms/step - accuracy: 0.7207 - loss: 0.7445 - val_accuracy: 0.6745 - val_loss: 0.8352\n",
      "Epoch 31/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 289ms/step - accuracy: 0.7092 - loss: 0.7560 - val_accuracy: 0.7075 - val_loss: 0.7557\n",
      "Epoch 32/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 288ms/step - accuracy: 0.7242 - loss: 0.7553 - val_accuracy: 0.5696 - val_loss: 1.4410\n",
      "Epoch 33/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 286ms/step - accuracy: 0.7274 - loss: 0.7433 - val_accuracy: 0.7288 - val_loss: 0.7472\n",
      "Epoch 34/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 282ms/step - accuracy: 0.7479 - loss: 0.6593 - val_accuracy: 0.7453 - val_loss: 0.7046\n",
      "Epoch 35/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 284ms/step - accuracy: 0.7433 - loss: 0.6828 - val_accuracy: 0.7488 - val_loss: 0.6674\n",
      "Epoch 36/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 286ms/step - accuracy: 0.7568 - loss: 0.6554 - val_accuracy: 0.7170 - val_loss: 0.7427\n",
      "Epoch 37/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 294ms/step - accuracy: 0.7408 - loss: 0.6859 - val_accuracy: 0.7476 - val_loss: 0.6963\n",
      "Epoch 38/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 285ms/step - accuracy: 0.7486 - loss: 0.6807 - val_accuracy: 0.7559 - val_loss: 0.6656\n",
      "Epoch 39/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 283ms/step - accuracy: 0.7597 - loss: 0.6659 - val_accuracy: 0.7653 - val_loss: 0.6451\n",
      "Epoch 40/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 299ms/step - accuracy: 0.7692 - loss: 0.6275 - val_accuracy: 0.7653 - val_loss: 0.6504\n",
      "Epoch 41/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 328ms/step - accuracy: 0.7597 - loss: 0.6280 - val_accuracy: 0.7358 - val_loss: 0.6938\n",
      "Epoch 42/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 319ms/step - accuracy: 0.7420 - loss: 0.6718 - val_accuracy: 0.7300 - val_loss: 0.7146\n",
      "Epoch 43/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 354ms/step - accuracy: 0.7733 - loss: 0.6027 - val_accuracy: 0.7653 - val_loss: 0.6223\n",
      "Epoch 44/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 322ms/step - accuracy: 0.7676 - loss: 0.6092 - val_accuracy: 0.7476 - val_loss: 0.6780\n",
      "Epoch 45/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 343ms/step - accuracy: 0.7682 - loss: 0.6177 - val_accuracy: 0.7594 - val_loss: 0.6532\n",
      "Epoch 46/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 348ms/step - accuracy: 0.7801 - loss: 0.5765 - val_accuracy: 0.7630 - val_loss: 0.6708\n",
      "Epoch 47/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 325ms/step - accuracy: 0.7752 - loss: 0.6068 - val_accuracy: 0.7429 - val_loss: 0.6921\n",
      "Epoch 48/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 303ms/step - accuracy: 0.7597 - loss: 0.6464 - val_accuracy: 0.7323 - val_loss: 0.6949\n",
      "Epoch 49/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 313ms/step - accuracy: 0.7817 - loss: 0.5727 - val_accuracy: 0.7300 - val_loss: 0.7020\n",
      "Epoch 50/50\n",
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 314ms/step - accuracy: 0.7706 - loss: 0.6019 - val_accuracy: 0.7594 - val_loss: 0.6751\n"
     ]
    }
   ],
   "source": [
    "# Ensure X_train and X_test are reshaped correctly\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.7594339847564697\n",
      "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 118ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.83      0.88        99\n",
      "           1       0.32      0.79      0.45        38\n",
      "           2       0.80      0.71      0.75       118\n",
      "           3       0.75      0.73      0.74       118\n",
      "           4       0.90      0.82      0.86       119\n",
      "           5       0.88      0.76      0.81       119\n",
      "           6       0.71      0.71      0.71       119\n",
      "           7       0.76      0.75      0.76       118\n",
      "\n",
      "    accuracy                           0.76       848\n",
      "   macro avg       0.76      0.76      0.75       848\n",
      "weighted avg       0.80      0.76      0.77       848\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test accuracy: {score[1]}\")\n",
    "\n",
    "# Generate classification report and confusion matrix\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
